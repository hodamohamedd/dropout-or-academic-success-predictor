{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "/usr/bin/env python\n",
        "coding: utf-8\n",
        "# Student Dropout Prediction: Model Training and Evaluation\n",
        "\n",
        "This notebook implements comprehensive model training and evaluation for student dropout prediction, including:\n",
        "1. Proper stratified validation strategy\n",
        "2. Multiple model types (Logistic Regression, Random Forest, Gradient Boosting, XGBoost)\n",
        "3. Systematic hyperparameter tuning\n",
        "4. Model evaluation with appropriate metrics\n",
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import os\n",
        "import optuna\n",
        "from optuna.visualization import plot_param_importances, plot_optimization_history\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries\n",
        "Set random seed for reproducibility\n",
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading dataset...\")\n",
        "data_path = '../data/dataset.csv'  # Update this path if needed\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset\n",
        "Display basic information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define target column\n",
        "Split features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = 'Target'  # Update this based on the actual target column name\n",
        "\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Target classes: {y.unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identify categorical and numerical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols[:10]}...\")  # Show first 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create preprocessing pipelines\n",
        "Combine preprocessing steps\n",
        "## 3. Train-Test Split with Stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "print(\"Preprocessing pipelines created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split data into train and test sets using stratification\n",
        "This ensures the class distribution is preserved in both sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check class distribution in train and test sets\n",
        "Visualize class distributions\n",
        "## 4. Model Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Class distribution in training set:\")\n",
        "train_dist = y_train.value_counts(normalize=True)\n",
        "print(train_dist)\n",
        "\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "test_dist = y_test.value_counts(normalize=True)\n",
        "print(test_dist)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "train_dist.plot(kind='bar', ax=ax1, title='Training Set Class Distribution')\n",
        "ax1.set_ylabel('Proportion')\n",
        "ax1.set_xlabel('Class')\n",
        "\n",
        "test_dist.plot(kind='bar', ax=ax2, title='Test Set Class Distribution')\n",
        "ax2.set_ylabel('Proportion')\n",
        "ax2.set_xlabel('Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../models/class_distribution.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define evaluation function\n",
        "For multi-class problems, we use 'macro' averaging\n",
        "Print classification report\n",
        "Plot confusion matrix\n",
        "## 5. Baseline Model: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate model performance with multiple metrics\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    \n",
        "    print(f\"\\nClassification Report for {model_name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=model.classes_, \n",
        "                yticklabels=model.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../models/{model_name}_confusion_matrix.png')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Logistic Regression pipeline\n",
        "Use cross-validation to evaluate baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"===== Training Baseline Model: Logistic Regression =====\")\n",
        "\n",
        "lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_scores = cross_val_score(lr_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model on the full training set\n",
        "Evaluate on test set\n",
        "Save the baseline model\n",
        "## 6. Random Forest with Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(\"Evaluating baseline model on test set:\")\n",
        "lr_metrics = evaluate_model(lr_pipeline, X_test, y_test, \"Logistic_Regression\")\n",
        "\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "joblib.dump(lr_pipeline, '../models/baseline_logistic_regression.joblib')\n",
        "print(\"Baseline model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the pipeline\n",
        "Define parameter grid for grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"===== Training Random Forest with Hyperparameter Tuning =====\")\n",
        "\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5],\n",
        "    'classifier__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "print(f\"Parameter grid: {param_grid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up grid search with cross-validation\n",
        "Fit grid search\n",
        "Print best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_rf = GridSearchCV(\n",
        "    rf_pipeline, \n",
        "    param_grid, \n",
        "    cv=cv, \n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Performing grid search for Random Forest...\")\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search_rf.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate best model on test set\n",
        "Save the best model\n",
        "## 7. Gradient Boosting with Optuna Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating best Random Forest model on test set:\")\n",
        "rf_metrics = evaluate_model(grid_search_rf.best_estimator_, X_test, y_test, \"Random_Forest\")\n",
        "\n",
        "joblib.dump(grid_search_rf.best_estimator_, '../models/tuned_random_forest.joblib')\n",
        "print(\"Tuned Random Forest model saved.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define an objective function for Optuna\n",
        "Define hyperparameters to optimize\n",
        "Create and preprocess the data\n",
        "Create and train the model\n",
        "Use cross-validation to evaluate\n",
        "Return the mean accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"===== Training Gradient Boosting with Optuna Hyperparameter Tuning =====\")\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'random_state': RANDOM_STATE\n",
        "    }\n",
        "    \n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    \n",
        "    model = GradientBoostingClassifier(**params)\n",
        "    \n",
        "    cv_scores = cross_val_score(\n",
        "        model, X_train_processed, y_train, \n",
        "        cv=cv, scoring='accuracy', n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    return cv_scores.mean()\n",
        "\n",
        "print(\"Objective function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a study object and optimize the objective function\n",
        "Print the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running Optuna optimization for Gradient Boosting...\")\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)  # Adjust n_trials as needed\n",
        "\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "print(f\"Best accuracy: {study.best_value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the final model with the best parameters\n",
        "Train the model on the full training set\n",
        "Evaluate on test set\n",
        "Save the model\n",
        "## 8. XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GradientBoostingClassifier(**study.best_params))\n",
        "])\n",
        "\n",
        "gb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(\"Evaluating best Gradient Boosting model on test set:\")\n",
        "gb_metrics = evaluate_model(gb_pipeline, X_test, y_test, \"Gradient_Boosting\")\n",
        "\n",
        "joblib.dump(gb_pipeline, '../models/tuned_gradient_boosting.joblib')\n",
        "print(\"Tuned Gradient Boosting model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the pipeline\n",
        "Define parameter grid for grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"===== Training XGBoost Model =====\")\n",
        "\n",
        "xgb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='mlogloss'))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [3, 6, 9],\n",
        "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'classifier__subsample': [0.8, 1.0],\n",
        "    'classifier__colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "print(f\"XGBoost parameter grid: {param_grid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up grid search with cross-validation\n",
        "Fit grid search\n",
        "Print best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_xgb = GridSearchCV(\n",
        "    xgb_pipeline, \n",
        "    param_grid, \n",
        "    cv=cv, \n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Performing grid search for XGBoost...\")\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search_xgb.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search_xgb.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate best model on test set\n",
        "Save the best model\n",
        "## 9. Model Comparison and Final Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating best XGBoost model on test set:\")\n",
        "xgb_metrics = evaluate_model(grid_search_xgb.best_estimator_, X_test, y_test, \"XGBoost\")\n",
        "\n",
        "joblib.dump(grid_search_xgb.best_estimator_, '../models/tuned_xgboost.joblib')\n",
        "print(\"Tuned XGBoost model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collect all model results\n",
        "Create comparison dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"===== Model Comparison =====\")\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': lr_metrics,\n",
        "    'Random Forest': rf_metrics,\n",
        "    'Gradient Boosting': gb_metrics,\n",
        "    'XGBoost': xgb_metrics\n",
        "}\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': list(models.keys()),\n",
        "    'Accuracy': [m['accuracy'] for m in models.values()],\n",
        "    'F1 Score (macro)': [m['f1_score'] for m in models.values()]\n",
        "})\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "comparison_sorted = comparison.sort_values('F1 Score (macro)', ascending=False)\n",
        "print(comparison_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot model comparison\n",
        "Create subplot with two charts\n",
        "Accuracy comparison\n",
        "F1 Score comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "comparison.set_index('Model')['Accuracy'].plot(kind='bar', ax=ax1, color='skyblue')\n",
        "ax1.set_title('Model Accuracy Comparison')\n",
        "ax1.set_ylabel('Accuracy Score')\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "comparison.set_index('Model')['F1 Score (macro)'].plot(kind='bar', ax=ax2, color='lightcoral')\n",
        "ax2.set_title('Model F1 Score Comparison')\n",
        "ax2.set_ylabel('F1 Score (Macro)')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../models/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identify and save the best model\n",
        "Save the best model with a standardized name\n",
        "Save best model for deployment\n",
        "## 10. Model Performance Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = comparison_sorted.iloc[0]['Model']\n",
        "best_accuracy = comparison_sorted.iloc[0]['Accuracy']\n",
        "best_f1 = comparison_sorted.iloc[0]['F1 Score (macro)']\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"üìä Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"üìä F1 Score (macro): {best_f1:.4f}\")\n",
        "\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    best_model = lr_pipeline\n",
        "elif best_model_name == 'Random Forest':\n",
        "    best_model = grid_search_rf.best_estimator_\n",
        "elif best_model_name == 'Gradient Boosting':\n",
        "    best_model = gb_pipeline\n",
        "else:  # XGBoost\n",
        "    best_model = grid_search_xgb.best_estimator_\n",
        "\n",
        "joblib.dump(best_model, '../models/best_model_for_deployment.joblib')\n",
        "print(f\"‚úÖ Best model saved for deployment!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"           MODEL TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"Dataset: {df.shape[0]} samples, {df.shape[1]-1} features\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Target classes: {list(y.unique())}\")\n",
        "print(f\"Cross-validation: {cv.n_splits}-fold stratified\")\n",
        "\n",
        "print(\"\\nModel Performance (Test Set):\")\n",
        "print(\"-\" * 40)\n",
        "for i, row in comparison_sorted.iterrows():\n",
        "    print(f\"{row['Model']:<20} | Acc: {row['Accuracy']:.4f} | F1: {row['F1 Score (macro)']:.4f}\")\n",
        "\n",
        "print(f\"\\nüéØ Best performing model: {best_model_name}\")\n",
        "print(f\"üìÅ Models saved to: ../models/\")\n",
        "print(f\"üìä Visualizations saved to: ../models/\")\n",
        "\n",
        "print(\"\\nFiles generated:\")\n",
        "print(\"- baseline_logistic_regression.joblib\")\n",
        "print(\"- tuned_random_forest.joblib\") \n",
        "print(\"- tuned_gradient_boosting.joblib\")\n",
        "print(\"- tuned_xgboost.joblib\")\n",
        "print(\"- best_model_for_deployment.joblib\")\n",
        "print(\"- model_comparison.png\")\n",
        "print(\"- [model_name]_confusion_matrix.png (for each model)\")\n",
        "\n",
        "print(\"\\n‚ú® Model training and evaluation complete! ‚ú®\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
